# 깃발을 잡아라! (mountain_car)

## 소개 
gymnasium의 mountain_car은 강화 학습 알고리즘을 테스트하고 개발하기 위해 사용되는 고전적인 강화 학습 문제입니다.
OpenAI Gym 라이브러리에서 찾을 수 있으며, Gym의 Mountain Car 이산형 및 연속형 두 가지 버전이 있습니다.
이번에 강화학습을 해볼 버전은 이산형 버전을 사용하여 학습을 진행하였습니다.
https://gymnasium.farama.org/environments/classic_control/mountain_car/

## 목표
목표는 차량이 오른쪽 언덕 꼭대기에있는 플래그에 도달하도록 전략적으로 가속시키는 것입니다. 

## 상태
2D 벡터이며, 첫 번째 요소는 위치, 두 번째 요소는 속도입니다.

## 행동 
action_size가 2로 설정되어 있으므로, 가능한 행동은 0 또는 1 두 가지입니다.

- Action 0:
0은 왼쪽으로 힘을 가하는 행동을 나타냅니다. 즉, 차량을 왼쪽으로 밀어내려는 행동입니다.

- Action 1:
1은 오른쪽으로 힘을 가하는 행동을 나타냅니다. 차량을 오른쪽으로 밀어내려는 행동입니다.

따라서, 에이전트는 각 타임 스텝에서 0 또는 1을 선택하여 차량에 가하는 힘의 방향을 결정합니다. 
이렇게 이산적인 행동으로 정의된 경우, 에이전트는 주어진 시점에서 어느 한 방향으로만 가속할 수 있습니다.

## 보상
 이번 학습에서는 에피소드가 종료되었을 때, 에피소드를 종료한 행동에 대한 패널티로 -100이 부여되며, 
 그 외의 경우에는 환경으로부터 직접 얻어진 보상값입니다.
 
* Mountain Car 환경에서 에피소드는 두 가지 경우에 종료됩니다:
1. 목표에 도달한 경우
에피소드는 차량이 환경에서 오른쪽 언덕 꼭대기의 목표 지점에 도달할 때 종료됩니다.
2. 최대 타임 스텝에 도달한 경우
에피소드는 최대 타임 스텝에 도달하면 종료됩니다. 일반적으로 환경에 설정된 최대 타임 스텝이 에피소드의 종료 조건 중 하나입니다.
이는 어떠한 이유로 에피소드가 목표에 도달하지 못하고 계속 진행되는 것을 방지하기 위한 것입니다.
--> 에이전트는 목표에 도달하거나 최대 타임 스텝에 도달하면 에피소드가 종료되고, 종료되면 해당 에피소드의 결과를 기반으로 학습이 이루어집니다.

* Mountain Car 문제에서는 차량이 초기 위치에서 오른쪽 언덕 꼭대기의 목표 지점에 도달하는 것이 목표이지만 
목표에 도달하였을때 패널티를 주는 이유는 다음과 같습니다:

1. 목표 도달 시 보상을 높게 설정하면?
목표에 도달하는 순간에 큰 보상을 부여하면, 강화 학습 알고리즘이 가능한 빨리 목표에 도달하는 방향으로 학습하려고 할 것입니다.
2. 왜 패널티를 부여하는가?
목표에 도달하기 위해서는 주어진 환경에서 적절한 행동을 학습해야 합니다.
하지만, Mountain Car에서는 차량이 최종 목표에 직접 가는 것이 불가능한 초기 상태에서 시작합니다.
초기 상태에서는 차량이 좌우로 움직여서 모멘텀을 얻어야만 목표에 도달할 수 있습니다.
3.패널티를 통한 격려
패널티를 주어 초기 상태에서 빠르게 목표에 도달하는 것이 어려울 경우, 
에이전트는 모멘텀을 얻기 위해 좌우로 움직이는 효과적인 전략을 학습하려고 할 것입니다.
따라서 패널티를 주는 것은 초기 상태에서 좋은 전략을 학습하도록 에이전트를 격려하고, 
초기 상태에서의 효율적인 행동을 찾도록 돕는 데에 기여합니다.

## 알고리즘 
이번 강화학습 실습에서는 Deep Q-Network (DQN) 알고리즘을 사용하고 있습니다. 
DQN은 강화 학습에서 Q-learning을 딥 뉴럴 네트워크로 확장한 것입니다. 
DQN은 경험 재생 (Experience Replay)과 목표 네트워크 (Target Network)를 사용하여 안정적인 학습을 도모합니다.

여러 가지 특징들이 DQN을 나타내고 있습니다:

1. 신경망 구조
build_model 메서드에서 정의된 신경망 모델을 사용하여 Q 함수를 근사화합니다.
2. Experience Replay
replay_memory 메서드를 통해 에이전트의 경험을 저장하고, train_replay 메서드를 통해 저장된 경험에서 무작위로 샘플을 추출하여 학습합니다.
3. 타겟 네트워크 (Target Network)
target_model과 update_target_model 메서드를 통해 사용되는 타겟 네트워크를 통해 안정성을 높입니다.
4. Epsilon-Greedy 정책
get_action 메서드에서 epsilon-greedy 정책을 사용하여 탐험과 활용을 조절합니다.

 
# 사용법
OpenAI Gym이 설치되어 있지 않은 경우, 다음 명령을 사용하여 설치합니다:

'''python
pip install gym
'''

# py 파일 실행하면됩니다.
